{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libs"
      ],
      "metadata": {
        "id": "g-lVjYzRHhl8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lQT3aJdLG-3n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "import scipy.stats #for t value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import Data & Create Dataframes"
      ],
      "metadata": {
        "id": "M9IRR5lfHl0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get logs-index\n",
        "df_logIndex = pd.read_excel(r'./logs-index.xlsx', index_col=None, header=1)\n",
        "\n",
        "# Remove empty rows and columns\n",
        "df_logIndex.dropna(axis=1, how='all', inplace=True) #delete cols with NaN\n",
        "df_logIndex.dropna(axis=0, how='all', inplace=True) #delete cols with NaN\n",
        "#df_logIndex_B.drop('Unnamed: 21',axis=1, inplace=True)\n",
        "\n",
        "# Split into batch dataframe\n",
        "df_Batch = df_logIndex[df_logIndex['Mode']=='batch']   #filter for batch\n",
        "df_Batch = df_Batch[df_logIndex['CPU']=='3000m']       #filter for 3000m cpu\n",
        "df_Batch['Time_scaled']=pd.to_numeric(df_Batch['Time_scaled']) \n",
        "#df_Batch = df_Batch.assign(b0=1)                         #add intercept values = 1\n",
        "\n",
        "# Split into simulated dataframe\n",
        "df_Sim = df_logIndex[df_logIndex['Mode']=='simulated'] #filter for simulated\n",
        "#df_Sim = df_Sim.assign(b0=1)                             #add intercept values = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wryzqulHHLkp",
        "outputId": "9e558c26-82ee-4ad1-e09b-9aa94bd4ad3f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-be7a73e255e7>:11: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  df_Batch = df_Batch[df_logIndex['CPU']=='3000m']       #filter for 3000m cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_Batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJvr6rNI_qaC",
        "outputId": "4467e681-fa77-4b7e-cc61-477fd3e6c0fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Purpose  Nodes    CPU Memory          Network  Epochs  Batch_size  \\\n",
            "1      NaN    1.0  3000m    2Gi  FashionMNISTCNN    80.0       128.0   \n",
            "2      NaN    1.0  3000m    2Gi  FashionMNISTCNN    80.0       128.0   \n",
            "3      NaN    1.0  3000m    2Gi  FashionMNISTCNN    80.0       128.0   \n",
            "4      NaN    2.0  3000m    2Gi  FashionMNISTCNN    80.0       128.0   \n",
            "5      NaN    2.0  3000m    2Gi  FashionMNISTCNN    80.0       128.0   \n",
            "6      NaN    2.0  3000m    2Gi  FashionMNISTCNN    80.0       128.0   \n",
            "7      NaN    4.0  3000m    2Gi  FashionMNISTCNN    80.0       128.0   \n",
            "8      NaN    4.0  3000m    2Gi  FashionMNISTCNN    80.0       128.0   \n",
            "9      NaN    4.0  3000m    2Gi  FashionMNISTCNN    80.0       128.0   \n",
            "10     NaN    1.0  3000m    2Gi  FashionMNISTCNN    80.0        32.0   \n",
            "11     NaN    1.0  3000m    2Gi  FashionMNISTCNN    80.0        32.0   \n",
            "12     NaN    1.0  3000m    2Gi  FashionMNISTCNN    80.0        32.0   \n",
            "13     NaN    2.0  3000m    2Gi  FashionMNISTCNN    80.0        32.0   \n",
            "14     NaN    2.0  3000m    2Gi  FashionMNISTCNN    80.0        32.0   \n",
            "15     NaN    2.0  3000m    2Gi  FashionMNISTCNN    80.0        32.0   \n",
            "16     NaN    4.0  3000m    2Gi  FashionMNISTCNN    80.0        32.0   \n",
            "17     NaN    4.0  3000m    2Gi  FashionMNISTCNN    80.0        32.0   \n",
            "18     NaN    4.0  3000m    2Gi  FashionMNISTCNN    80.0        32.0   \n",
            "19     NaN    1.0  3000m    2Gi  FashionMNISTCNN    40.0        32.0   \n",
            "20     NaN    1.0  3000m    2Gi  FashionMNISTCNN    40.0        32.0   \n",
            "21     NaN    1.0  3000m    2Gi  FashionMNISTCNN    40.0        32.0   \n",
            "22     NaN    2.0  3000m    2Gi  FashionMNISTCNN    40.0        32.0   \n",
            "23     NaN    2.0  3000m    2Gi  FashionMNISTCNN    40.0        32.0   \n",
            "24     NaN    2.0  3000m    2Gi  FashionMNISTCNN    40.0        32.0   \n",
            "25     NaN    4.0  3000m    2Gi  FashionMNISTCNN    40.0        32.0   \n",
            "26     NaN    4.0  3000m    2Gi  FashionMNISTCNN    40.0        32.0   \n",
            "27     NaN    4.0  3000m    2Gi  FashionMNISTCNN    40.0        32.0   \n",
            "28     NaN    1.0  3000m    2Gi  FashionMNISTCNN    40.0       128.0   \n",
            "29     NaN    1.0  3000m    2Gi  FashionMNISTCNN    40.0       128.0   \n",
            "30     NaN    1.0  3000m    2Gi  FashionMNISTCNN    40.0       128.0   \n",
            "31     NaN    2.0  3000m    2Gi  FashionMNISTCNN    40.0       128.0   \n",
            "32     NaN    2.0  3000m    2Gi  FashionMNISTCNN    40.0       128.0   \n",
            "33     NaN    2.0  3000m    2Gi  FashionMNISTCNN    40.0       128.0   \n",
            "34     NaN    4.0  3000m    2Gi  FashionMNISTCNN    40.0       128.0   \n",
            "35     NaN    4.0  3000m    2Gi  FashionMNISTCNN    40.0       128.0   \n",
            "36     NaN    4.0  3000m    2Gi  FashionMNISTCNN    40.0       128.0   \n",
            "84     hw2    1.0  3000m    2Gi  FashionMNISTCNN    80.0       128.0   \n",
            "85     hw2    2.0  3000m    2Gi  FashionMNISTCNN    80.0       128.0   \n",
            "86     hw2    4.0  3000m    2Gi  FashionMNISTCNN    80.0       128.0   \n",
            "\n",
            "       Loss_function Optimizer Dataset  ... inter-arrival time(s)  \\\n",
            "1   CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "2   CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "3   CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "4   CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "5   CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "6   CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "7   CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "8   CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "9   CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "10  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "11  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "12  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "13  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "14  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "15  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "16  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "17  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "18  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "19  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "20  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "21  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "22  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "23  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "24  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "25  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "26  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "27  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "28  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "29  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "30  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "31  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "32  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "33  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "34  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "35  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "36  CrossEntropyLoss      adam   mnist  ...                   NaN   \n",
            "84  CrossEntropyLoss       SGD   mnist  ...                   NaN   \n",
            "85  CrossEntropyLoss       SGD   mnist  ...                   NaN   \n",
            "86  CrossEntropyLoss       SGD   mnist  ...                   NaN   \n",
            "\n",
            "    Replication  Duration                                           Log_file  \\\n",
            "1           1.0       NaN                                                NaN   \n",
            "2           2.0       NaN                                                NaN   \n",
            "3           3.0       NaN                                                NaN   \n",
            "4           1.0       NaN  events.out.tfevents.1666444756.trainjob-998d83...   \n",
            "5           2.0       NaN  https://drive.google.com/file/d/1V5nQAisjQC__f...   \n",
            "6           3.0       NaN  https://drive.google.com/file/d/1P2W9MqWuR89p2...   \n",
            "7           1.0       NaN  events.out.tfevents.1666338562.trainjob-ab2c22...   \n",
            "8           2.0       NaN  https://drive.google.com/file/d/1NktAJchI-5ejP...   \n",
            "9           3.0       NaN  https://drive.google.com/file/d/1ZeuYKHeKvt_ak...   \n",
            "10          1.0       NaN  events.out.tfevents.1666729905.trainjob-7ac690...   \n",
            "11          2.0       NaN  events.out.tfevents.1666764731.trainjob-ed3ed1...   \n",
            "12          3.0       NaN  events.out.tfevents.1666767326.trainjob-d641b8...   \n",
            "13          1.0       NaN  events.out.tfevents.1666728271.trainjob-6c096f...   \n",
            "14          2.0       NaN  events.out.tfevents.1666771395.trainjob-38bb09...   \n",
            "15          3.0       NaN  events.out.tfevents.1666769547.trainjob-3a4f67...   \n",
            "16          1.0       NaN  events.out.tfevents.1666726015.trainjob-45f9c9...   \n",
            "17          2.0       NaN  events.out.tfevents.1666773781.trainjob-645282...   \n",
            "18          3.0       NaN  events.out.tfevents.1666775040.trainjob-abdc68...   \n",
            "19          1.0       NaN  events.out.tfevents.1666722581.trainjob-c670ac...   \n",
            "20          2.0       NaN  https://drive.google.com/file/d/1FX5FraiDCGRp6...   \n",
            "21          3.0       NaN  events.out.tfevents.1666780002.trainjob-cfc61f...   \n",
            "22          1.0       NaN  events.out.tfevents.1666724027.trainjob-67587f...   \n",
            "23          2.0       NaN  events.out.tfevents.1666777895.trainjob-3757aa...   \n",
            "24          3.0       NaN  events.out.tfevents.1666778612.trainjob-0d754b...   \n",
            "25          1.0       NaN  events.out.tfevents.1666725254.trainjob-0911e5...   \n",
            "26          2.0       NaN  events.out.tfevents.1666777240.trainjob-02d41d...   \n",
            "27          3.0       NaN  events.out.tfevents.1666776354.trainjob-978fda...   \n",
            "28          1.0       NaN  https://drive.google.com/file/d/1ZnZ62HyO88uTe...   \n",
            "29          2.0       NaN  https://drive.google.com/file/d/1s6Yg23BoFekkl...   \n",
            "30          3.0       NaN  https://drive.google.com/file/d/1bEgVglyP5TthC...   \n",
            "31          1.0       NaN  https://drive.google.com/file/d/1YOGUuY3OX7giu...   \n",
            "32          2.0       NaN  https://drive.google.com/file/d/1FVPUg-qpk5k0w...   \n",
            "33          3.0       NaN  https://drive.google.com/file/d/1Ukzd0kwqLFPbU...   \n",
            "34          1.0       NaN  https://drive.google.com/file/d/15CMhvYmFSTr6C...   \n",
            "35          2.0       NaN  https://drive.google.com/file/d/1rG1zIoZx9IM1G...   \n",
            "36          3.0       NaN  https://drive.google.com/file/d/1L0LOiIHGrVhnS...   \n",
            "84          1.0       NaN  events.out.tfevents.1666447127.trainjob-8f4e02...   \n",
            "85          1.0       NaN  https://drive.google.com/file/d/1jb-YOq6H16XJJ...   \n",
            "86          1.0       NaN  https://drive.google.com/file/d/1ennTm1Ds2zTn1...   \n",
            "\n",
            "       Service_time Time_scaled Response_time  Waiting_time(s) E[S]  \\\n",
            "1   00:19:29.150000         NaN           NaN              NaN  NaN   \n",
            "2   00:19:47.823000         NaN           NaN              NaN  NaN   \n",
            "3   00:26:30.507000         NaN           NaN              NaN  NaN   \n",
            "4   00:24:37.347000   45.628013           NaN              NaN  NaN   \n",
            "5   00:14:17.863000   26.495170           NaN              NaN  NaN   \n",
            "6   00:14:28.229000   26.815339           NaN              NaN  NaN   \n",
            "7   00:13:01.032000   24.122233           NaN              NaN  NaN   \n",
            "8   00:11:16.864000   20.905021           NaN              NaN  NaN   \n",
            "9   00:11:20.477000   21.016593           NaN              NaN  NaN   \n",
            "10  00:25:58.399000   48.131288           NaN              NaN  NaN   \n",
            "11  00:26:02.760000   48.265982           NaN              NaN  NaN   \n",
            "12  00:26:30.981000   49.137592           NaN              NaN  NaN   \n",
            "13  00:22:08.070000   41.017575           NaN              NaN  NaN   \n",
            "14  00:22:42.951000   42.094862           NaN              NaN  NaN   \n",
            "15  00:23:29.163000   43.522142           NaN              NaN  NaN   \n",
            "16  00:15:32.334000   28.795227           NaN              NaN  NaN   \n",
            "17  00:15:29.863000   28.718921           NaN              NaN  NaN   \n",
            "18  00:15:54.013000   29.464781           NaN              NaN  NaN   \n",
            "19  00:16:24.119000   30.394592           NaN              NaN  NaN   \n",
            "20  00:13:13.802000   24.516646           NaN              NaN  NaN   \n",
            "21  00:13:10.070000   24.401375           NaN              NaN  NaN   \n",
            "22  00:11:16.385000   20.890221           NaN              NaN  NaN   \n",
            "23  00:09:02.141000   16.744072           NaN              NaN  NaN   \n",
            "24  00:09:05.645000   16.852312           NaN              NaN  NaN   \n",
            "25  00:07:19.873000   13.585527           NaN              NaN  NaN   \n",
            "26  00:07:54.082000   14.642079           NaN              NaN  NaN   \n",
            "27  00:07:48.610000   14.473053           NaN              NaN  NaN   \n",
            "28  00:10:04.042000   18.655896           NaN              NaN  NaN   \n",
            "29  00:10:09.223000   18.815907           NaN              NaN  NaN   \n",
            "30  00:09:49.198000   18.197427           NaN              NaN  NaN   \n",
            "31  00:06:54.129000   12.790403           NaN              NaN  NaN   \n",
            "32  00:07:06.425000   13.170172           NaN              NaN  NaN   \n",
            "33  00:06:54.110000   12.789827           NaN              NaN  NaN   \n",
            "34  00:05:35.150000   10.351145           NaN              NaN  NaN   \n",
            "35  00:05:28.737000   10.153067           NaN              NaN  NaN   \n",
            "36  00:05:18.543000    9.838221           NaN              NaN  NaN   \n",
            "84  00:34:53.749000   64.665622           NaN              NaN  NaN   \n",
            "85  00:20:25.291000    1.040486           NaN              NaN  NaN   \n",
            "86  00:19:37.614000    1.000000           NaN              NaN  NaN   \n",
            "\n",
            "               E[R]  \n",
            "1               NaN  \n",
            "2               NaN  \n",
            "3               NaN  \n",
            "4               NaN  \n",
            "5               NaN  \n",
            "6               NaN  \n",
            "7               NaN  \n",
            "8               NaN  \n",
            "9   00:00:32.378000  \n",
            "10  00:34:53.749000  \n",
            "11              NaN  \n",
            "12              NaN  \n",
            "13              NaN  \n",
            "14              NaN  \n",
            "15              NaN  \n",
            "16              NaN  \n",
            "17              NaN  \n",
            "18              NaN  \n",
            "19              NaN  \n",
            "20              NaN  \n",
            "21              NaN  \n",
            "22              NaN  \n",
            "23              NaN  \n",
            "24              NaN  \n",
            "25              NaN  \n",
            "26              NaN  \n",
            "27              NaN  \n",
            "28              NaN  \n",
            "29              NaN  \n",
            "30              NaN  \n",
            "31              NaN  \n",
            "32              NaN  \n",
            "33              NaN  \n",
            "34              NaN  \n",
            "35              NaN  \n",
            "36              NaN  \n",
            "84              NaN  \n",
            "85              NaN  \n",
            "86              NaN  \n",
            "\n",
            "[39 rows x 22 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Model for Batch experiments"
      ],
      "metadata": {
        "id": "GzMWxOIdHwz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANOVA Analysis for Regression Model**"
      ],
      "metadata": {
        "id": "MvDRv_XHuBa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#link for Ordinary Least Square (OLS) Method for Linear Regression by hand: https://medium.com/analytics-vidhya/ordinary-least-square-ols-method-for-linear-regression-ef8ca10aadfc\n",
        "\n",
        "# Create regression model\n",
        "model1 = ols(\"\"\"Time_scaled  ~ Nodes + Epochs + C(Batch_size) + \n",
        "               Nodes:Epochs + Nodes:C(Batch_size) + Epochs:C(Batch_size) + \n",
        "               Nodes:Epochs:C(Batch_size)\"\"\", data=df_Batch).fit() #C() --> treat as categorical variable\n",
        "\n",
        "# Anova analysis of model               \n",
        "anova_results = sm.stats.anova_lm(model1,tpy=3) #typ 2 assumes no significant interactions and is morepowerful than type 3 if this is true\n",
        "# note: if the data is balanced (equal sample size for each group), Type 1, 2, and 3 sums of squares will produce similar results."
      ],
      "metadata": {
        "id": "G0FrNhHCW__j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dir(model1._results) # list of attributes of model results\n",
        "dir(model1)\n",
        "#model1.get_influence().summary_frame()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh0SgO9_CS7Z",
        "outputId": "de202b6e-ab5a-4edb-caeb-a9b67fa09c19"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['HC0_se',\n",
              " 'HC1_se',\n",
              " 'HC2_se',\n",
              " 'HC3_se',\n",
              " '_HCCM',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_abat_diagonal',\n",
              " '_cache',\n",
              " '_data_attr',\n",
              " '_data_in_cache',\n",
              " '_get_robustcov_results',\n",
              " '_is_nested',\n",
              " '_use_t',\n",
              " '_wexog_singular_values',\n",
              " 'aic',\n",
              " 'bic',\n",
              " 'bse',\n",
              " 'centered_tss',\n",
              " 'compare_f_test',\n",
              " 'compare_lm_test',\n",
              " 'compare_lr_test',\n",
              " 'condition_number',\n",
              " 'conf_int',\n",
              " 'conf_int_el',\n",
              " 'cov_HC0',\n",
              " 'cov_HC1',\n",
              " 'cov_HC2',\n",
              " 'cov_HC3',\n",
              " 'cov_kwds',\n",
              " 'cov_params',\n",
              " 'cov_type',\n",
              " 'df_model',\n",
              " 'df_resid',\n",
              " 'eigenvals',\n",
              " 'el_test',\n",
              " 'ess',\n",
              " 'f_pvalue',\n",
              " 'f_test',\n",
              " 'fittedvalues',\n",
              " 'fvalue',\n",
              " 'get_influence',\n",
              " 'get_prediction',\n",
              " 'get_robustcov_results',\n",
              " 'info_criteria',\n",
              " 'initialize',\n",
              " 'k_constant',\n",
              " 'llf',\n",
              " 'load',\n",
              " 'model',\n",
              " 'mse_model',\n",
              " 'mse_resid',\n",
              " 'mse_total',\n",
              " 'nobs',\n",
              " 'normalized_cov_params',\n",
              " 'outlier_test',\n",
              " 'params',\n",
              " 'predict',\n",
              " 'pvalues',\n",
              " 'remove_data',\n",
              " 'resid',\n",
              " 'resid_pearson',\n",
              " 'rsquared',\n",
              " 'rsquared_adj',\n",
              " 'save',\n",
              " 'scale',\n",
              " 'ssr',\n",
              " 'summary',\n",
              " 'summary2',\n",
              " 't_test',\n",
              " 't_test_pairwise',\n",
              " 'tvalues',\n",
              " 'uncentered_tss',\n",
              " 'use_t',\n",
              " 'wald_test',\n",
              " 'wald_test_terms',\n",
              " 'wresid']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n * Anova results: \")\n",
        "print(anova_results)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pezhq5QH9hgo",
        "outputId": "5c9a4ee5-8c56-4122-a14e-c31abc801bc5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " * Anova results: \n",
            "                              df       sum_sq      mean_sq          F  \\\n",
            "C(Batch_size)                1.0   891.994915   891.994915  11.299584   \n",
            "Nodes                        1.0  1315.557436  1315.557436  16.665175   \n",
            "Nodes:C(Batch_size)          1.0     3.012153     3.012153   0.038157   \n",
            "Epochs                       1.0  2684.843590  2684.843590  34.010973   \n",
            "Epochs:C(Batch_size)         1.0    75.708299    75.708299   0.959055   \n",
            "Nodes:Epochs                 1.0   301.777143   301.777143   3.822843   \n",
            "Nodes:Epochs:C(Batch_size)   1.0    59.226887    59.226887   0.750272   \n",
            "Residual                    28.0  2210.334294    78.940511        NaN   \n",
            "\n",
            "                              PR(>F)  \n",
            "C(Batch_size)               0.002255  \n",
            "Nodes                       0.000337  \n",
            "Nodes:C(Batch_size)         0.846538  \n",
            "Epochs                      0.000003  \n",
            "Epochs:C(Batch_size)        0.335811  \n",
            "Nodes:Epochs                0.060607  \n",
            "Nodes:Epochs:C(Batch_size)  0.393752  \n",
            "Residual                         NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"* Regression Model Summary: \") #link on how to interpret: https://www.geeksforgeeks.org/interpreting-the-results-of-linear-regression-using-ols-summary/\n",
        "print(model1.summary()) \n",
        "\n",
        "print(\"\\n\\n * Regression Parameter Estimates: \") \n",
        "print(model1._results.params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb-YswGhQHuD",
        "outputId": "085367ac-6a77-428e-ff4c-1cd4e6da48b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Regression Model Summary: \n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:            Time_scaled   R-squared:                       0.707\n",
            "Model:                            OLS   Adj. R-squared:                  0.634\n",
            "Method:                 Least Squares   F-statistic:                     9.649\n",
            "Date:                Wed, 05 Apr 2023   Prob (F-statistic):           4.59e-06\n",
            "Time:                        09:51:31   Log-Likelihood:                -125.19\n",
            "No. Observations:                  36   AIC:                             266.4\n",
            "Df Residuals:                      28   BIC:                             279.1\n",
            "Df Model:                           7                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=======================================================================================================\n",
            "                                          coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "Intercept                               1.6828     14.048      0.120      0.906     -27.094      30.459\n",
            "C(Batch_size)[T.128.0]                -14.1403     20.414     -0.693      0.494     -55.956      27.675\n",
            "Nodes                                  -1.0140      5.310     -0.191      0.850     -11.890       9.862\n",
            "Nodes:C(Batch_size)[T.128.0]            5.3762      7.588      0.709      0.484     -10.167      20.920\n",
            "Epochs                                  0.6680      0.222      3.007      0.006       0.213       1.123\n",
            "Epochs:C(Batch_size)[T.128.0]           0.1424      0.335      0.425      0.674      -0.544       0.829\n",
            "Nodes:Epochs                           -0.0688      0.084     -0.820      0.419      -0.241       0.103\n",
            "Nodes:Epochs:C(Batch_size)[T.128.0]    -0.1055      0.122     -0.866      0.394      -0.355       0.144\n",
            "==============================================================================\n",
            "Omnibus:                       24.501   Durbin-Watson:                   2.072\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               85.375\n",
            "Skew:                          -1.264   Prob(JB):                     2.89e-19\n",
            "Kurtosis:                      10.108   Cond. No.                     3.57e+03\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 3.57e+03. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n",
            "\n",
            "\n",
            " * Regression Parameter Estimates: \n",
            "[  1.68282856 -14.14034185  -1.01400333   5.37620932   0.66797584\n",
            "   0.14240546  -0.06883668  -0.10552511]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goodness of Fit**"
      ],
      "metadata": {
        "id": "Nb0f9NWJ3pi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Factor Analysis\n",
        "print(\"\\nFACTOR ANALYSIS:\")\n",
        "# Determine statistical significance of factors (F-test factors)\n",
        "print(\"\\n___SIGNIFICANCE & IMPORTANCE________________________________\")\n",
        "f_significance = 0.05\n",
        "factor_P_val = anova_results['PR(>F)'].values\n",
        "df_factor = pd.DataFrame(data=factor_P_val, index = anova_results.index, columns=['P_val']) # if P val < significance -> stat. sig.\n",
        "df_factor['significance(0.05)']= factor_P_val<f_significance\n",
        "SST =  sum(anova_results['sum_sq'].values)\n",
        "df_factor['% variation']= np.round(100*anova_results['sum_sq'].values/SST)\n",
        "\n",
        "print(df_factor.sort_values(by='% variation', ascending=False))\n",
        "\n",
        "# Determine a definite difference in performance of level of factors (T-test factors)\n",
        "# -- NB: estimate factor +- (t_value)(std of estimate factor)\n",
        "# -- NB: estimate factor level = avg y for factor level - y tot avg\n",
        "\n",
        "# -- Determine avg y and std of y for factor levels\n",
        "# ---- variable declaration\n",
        "nodes_levels = df_Batch.drop_duplicates(inplace=False, subset=['Nodes'])['Nodes']._values #[1,2,4]\n",
        "epochs_levels = df_Batch.drop_duplicates(inplace=False, subset=['Epochs'])['Epochs']._values #[40,80]\n",
        "batchsize_levels = df_Batch.drop_duplicates(inplace=False, subset=['Batch_size'])['Batch_size']._values #[32,128]\n",
        "factor_se2 = anova_results['sum_sq'].values[-1] / anova_results['df'].values[-1]\n",
        "factor_df_y = len(nodes_levels)*len(epochs_levels)*len(batchsize_levels)*3 # df_y = a*b*c*replications\n",
        "\n",
        "# ---- calculations\n",
        "y_avg_nodes = []\n",
        "y_std_nodes = []\n",
        "for i in nodes_levels:\n",
        "  y_avg_nodes += [df_Batch[df_Batch['Nodes']==i]['Time_scaled'].mean()] #avg y for nodes = i\n",
        "  y_std_nodes += [(factor_se2*(len(nodes_levels)-1))/factor_df_y]\n",
        "\n",
        "y_avg_epochs = []\n",
        "y_std_epochs = []\n",
        "for i in epochs_levels:\n",
        "  y_avg_epochs += [df_Batch[df_Batch['Epochs']==i]['Time_scaled'].mean()] #avg y for epochs = i\n",
        "  y_std_epochs += [(factor_se2*(len(epochs_levels)-1))/factor_df_y]\n",
        "\n",
        "y_avg_batchsize = []\n",
        "y_std_batchsize = []\n",
        "for i in batchsize_levels:\n",
        "  y_avg_batchsize += [df_Batch[df_Batch['Batch_size']==i]['Time_scaled'].mean()] #avg y for epochs = i\n",
        "  y_std_batchsize += [(factor_se2*(len(batchsize_levels)-1))/factor_df_y]\n",
        "\n",
        "# -- Determine estimates for factor levels\n",
        "y_est_nodes = y_avg_nodes - df_Batch['Time_scaled'].mean()\n",
        "y_est_epochs = y_avg_epochs - df_Batch['Time_scaled'].mean()\n",
        "y_est_batchsize = y_avg_batchsize - df_Batch['Time_scaled'].mean()\n",
        "\n",
        "# -- Determine confidence interval \n",
        "t_significance = 0.10 # 90% confidence\n",
        "df_error = anova_results['df'].values[-1]\n",
        "t_value = scipy.stats.t.ppf(q=1-(t_significance/2),df=df_error) #two tailed \n",
        "\n",
        "CI_est_nodes_l = y_est_nodes - np.multiply(y_std_nodes,t_value)\n",
        "CI_est_nodes_u = y_est_nodes + np.multiply(y_std_nodes,t_value)\n",
        "print(\"\\n___NODE CONFIDENCE INTERVAL________________________________\")\n",
        "print(\"Nodes levels order: \", nodes_levels)\n",
        "print(\"Nodes CI lower bound: \", CI_est_nodes_l)\n",
        "print(\"Nodes CI upper bound: \", CI_est_nodes_u)\n",
        "print(\"Definite difference at \"+str(100*(1-t_significance))+\"% CI level: \",(CI_est_nodes_l*CI_est_nodes_u)>0) #if intervals contain 0 result negative\n",
        "\n",
        "CI_est_epochs_l = y_est_epochs - np.multiply(y_std_epochs,t_value)\n",
        "CI_est_epochs_u = y_est_epochs + np.multiply(y_std_epochs,t_value)\n",
        "print(\"\\n___EPOCHS CONFIDENCE INTERVAL______________________________\")\n",
        "print(\"Epochs levels order: \", epochs_levels)\n",
        "print(\"Epochs CI lower bound: \", CI_est_epochs_l)\n",
        "print(\"Epochs CI upper bound: \", CI_est_epochs_u)\n",
        "print(\"Definite difference at \"+str(100*(1-t_significance))+\"% CI level: \",(CI_est_epochs_l*CI_est_epochs_u)>0) #if intervals contain 0 result negative\n",
        "\n",
        "CI_est_batchsize_l = y_est_batchsize - np.multiply(y_std_batchsize,t_value)\n",
        "CI_est_batchsize_u = y_est_batchsize + np.multiply(y_std_batchsize,t_value)\n",
        "print(\"\\n___BATCH SIZE CONFIDENCE INTERVAL__________________________\")\n",
        "print(\"Batch size levels order: \", batchsize_levels)\n",
        "print(\"Batch size CI lower bound: \", CI_est_batchsize_l)\n",
        "print(\"Batch size CI upper bound: \", CI_est_batchsize_u)\n",
        "print(\"Definite difference at \"+str(100*(1-t_significance))+\"% CI level: \",(CI_est_batchsize_l*CI_est_batchsize_u)>0) #if intervals contain 0 result negative\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWXxOS9WzUc7",
        "outputId": "db9f3e08-9ebd-4342-95f2-be9b9c0123ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FACTOR ANALYSIS:\n",
            "\n",
            "___SIGNIFICANCE & IMPORTANCE________________________________\n",
            "                               P_val  significance(0.05)  % variation\n",
            "Epochs                      0.000003                True         36.0\n",
            "Residual                         NaN               False         29.0\n",
            "Nodes                       0.000337                True         17.0\n",
            "C(Batch_size)               0.002255                True         12.0\n",
            "Nodes:Epochs                0.060607               False          4.0\n",
            "Epochs:C(Batch_size)        0.335811               False          1.0\n",
            "Nodes:Epochs:C(Batch_size)  0.393752               False          1.0\n",
            "Nodes:C(Batch_size)         0.846538               False          0.0\n",
            "\n",
            "___NODE CONFIDENCE INTERVAL________________________________\n",
            "Nodes levels order:  [1. 2. 4.]\n",
            "Nodes CI lower bound:  [  2.27725842  -7.63708255 -14.77436922]\n",
            "Nodes CI upper bound:  [17.19816335  7.28382239  0.14653571]\n",
            "Definite difference at 90.0% CI level:  [ True False False]\n",
            "\n",
            "___EPOCHS CONFIDENCE INTERVAL______________________________\n",
            "Epochs levels order:  [80. 40.]\n",
            "Epochs CI lower bound:  [  4.31352101 -11.77397347]\n",
            "Epochs CI upper bound:  [11.77397347 -4.31352101]\n",
            "Definite difference at 90.0% CI level:  [ True  True]\n",
            "\n",
            "___BATCH SIZE CONFIDENCE INTERVAL__________________________\n",
            "Batch size levels order:  [128.  32.]\n",
            "Batch size CI lower bound:  [-8.70794022  1.24748775]\n",
            "Batch size CI upper bound:  [-1.24748775  8.70794022]\n",
            "Definite difference at 90.0% CI level:  [ True  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Model Analysis \n",
        "print(\"\\nMODEL ANALYSIS:\")\n",
        "\n",
        "# Determine statistical significance of regression model (F-test model)\n",
        "# --MSR/MSE  (Null hypothesis is that y doesnt depend on any factor x)\n",
        "f_significance = 0.05\n",
        "print(\"\\n___STATISTICAL SIGNIFICANCE________________________________\")\n",
        "print(\"Given significance of \"+str(f_significance)+\" the statistically significance of the model evaluates to: \", model1.f_pvalue<f_significance)# if P val < significance -> stat. sig.\n",
        "\n",
        "# Coefficient of Determination\n",
        "print(\"\\n___COEFF OF DETERMINATION__________________________________\")\n",
        "print(\"Coefficient of Determination (R^2) = \",np.round(model1._results.rsquared*100),\"%\") # % of variation of y explained by regression (importance)\n",
        "\n",
        "# Confidence interval of regression parameters (T-test factors)\n",
        "# --estimate b +- (t_value)(std of estimate b)\n",
        "t_significance = 0.10 # 90% confidence\n",
        "df_error_model = model1.df_resid\n",
        "t_value_model = scipy.stats.t.ppf(q=1-(t_significance/2),df=df_error_model) #two tailed \n",
        "model_se2 = model1.mse_resid\n",
        "model_df_y = model1._results.nobs\n",
        "model_std_param = (model_se2 * 1)/model_df_y #df of regression parameter is 1 \n",
        "\n",
        "CI_b = pd.DataFrame(data=model1.params - (t_value_model * model_std_param), columns=['lower_bound'])\n",
        "CI_b['upper_bound']= model1.params + (t_value_model * model_std_param )\n",
        "CI_b[\"t_test_pass\"] = (CI_b['lower_bound']*CI_b['upper_bound'])>0 #if intervals contain 0 result negative\n",
        "\n",
        "print(\"\\n___REGRESSION PARAM (b) CONFIDENCE INTERVAL________________\")\n",
        "print(CI_b)\n",
        "\n",
        "# Confidence interval of predicted response\n",
        "# --estimate y +- (t_value)(std of estimate y)\n",
        "print(\"\\n___PREDICTED RESPONSE CONFIDENCE INTERVAL__________________\")\n",
        "#print(\"Predicted value =\", model1.predict()) \n",
        "#print(model1.get_prediction().summary_frame().mean_se)  #to get the std err (se) of predicted values\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OLz7mmx3jq-",
        "outputId": "1ef3a8a6-7f0c-4308-df45-2a5f7d654e1e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MODEL ANALYSIS:\n",
            "\n",
            "___STATISTICAL SIGNIFICANCE________________________________\n",
            "Given significance of 0.05 the statistically significance of the model evaluates to:  True\n",
            "\n",
            "___COEFF OF DETERMINATION__________________________________\n",
            "Coefficient of Determination (R^2) =  71.0 %\n",
            "\n",
            "___REGRESSION PARAM (b) CONFIDENCE INTERVAL________________\n",
            "                                     lower_bound  upper_bound  t_test_pass\n",
            "Intercept                              -2.047398     5.413055        False\n",
            "C(Batch_size)[T.128.0]                -17.870568   -10.410116         True\n",
            "Nodes                                  -4.744230     2.716223        False\n",
            "Nodes:C(Batch_size)[T.128.0]            1.645983     9.106436         True\n",
            "Epochs                                 -3.062250     4.398202        False\n",
            "Epochs:C(Batch_size)[T.128.0]          -3.587821     3.872632        False\n",
            "Nodes:Epochs                           -3.799063     3.661390        False\n",
            "Nodes:Epochs:C(Batch_size)[T.128.0]    -3.835751     3.624701        False\n",
            "\n",
            "___PREDICTED RESPONSE CONFIDENCE INTERVAL__________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Based on regression example at end of slides 2\n",
        "'''\n",
        "#Estimating model parameters for [y] = [X][b] + [e]\n",
        "\n",
        "\n",
        "TO DO: Calculate averages over replications\n",
        "\n",
        "NB: Model doesnt calculate for interactions between factors\n",
        "\n",
        "y = df_Batch[['Time_scaled']].to_numpy()\n",
        "X = df_Batch[['b0','Nodes','Epochs','Batch_size']].to_numpy() \n",
        "\n",
        "C= np.dot(X.transpose(),X) #(X^T X)\n",
        "#if determinant not 0\n",
        "if np.linalg.det(C) != 0:\n",
        "  C=np.lingalg.inv(C) \n",
        "else: \n",
        "  C = C#*0 #set to 0 to indicate invalid C val\n",
        "\n",
        "D =  np.dot(X.transpose(),y)\n",
        "b = np.dot(C, D)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "8pwabjGpYyDI",
        "outputId": "7f334e64-79c6-4d61-e78f-ba3df01b7e81"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#Estimating model parameters for [y] = [X][b] + [e]\\n\\n\\nTO DO: Calculate averages over replications\\n\\nNB: Model doesnt calculate for interactions between factors\\n\\ny = df_Batch[['Time_scaled']].to_numpy()\\nX = df_Batch[['b0','Nodes','Epochs','Batch_size']].to_numpy() \\n\\nC= np.dot(X.transpose(),X) #(X^T X)\\n#if determinant not 0\\nif np.linalg.det(C) != 0:\\n  C=np.lingalg.inv(C) \\nelse: \\n  C = C#*0 #set to 0 to indicate invalid C val\\n\\nD =  np.dot(X.transpose(),y)\\nb = np.dot(C, D)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDHP8bvtiyt8"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}